# Part2 深度学习基础

## 1.深度学习基本步骤

### 1.1 前向传播

前向传播是已知网络参数，输入数据，求得对应的结果和损失函数值的过程。

**非最后一层：**每一层一般是先带入网络求值，然后再带入激活函数求值。比如输入为X，隐藏层为MLP，参数为W,b，激活函数为ReLU，则输出为Y=ReLU(WX+b)。激活函数为了提供非线性。

**最后一层：**对于最常见的分类问题来说，最后一层通常是**全连接层+softmax+求损失函数**。

全连接层将输入映射到一个n维向量，n是类别数目；

之后对n维向量做softmax，结果还是n维向量，不过实现了“赢者通吃”，原先n维向量的结果差异被显著放大了。
$$
g(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{k}e^{z_j}}\\
因为softmax可能导致数值溢出，常常分子分母都除以e^{z_{max}}
$$
最后用n维向量结果和标签y做交叉熵求损失函数。标签需要经过one-hot encoding（独热编码），也就是这样：如果有n个类别，某个结果y属于第i类，则这个结果的one-hot encoding有n维，其第i维是1，其余维是0。交叉熵的结果是一个数值，就是损失函数。
$$
-\sum_{j=1}^{k}q_jlog(p_j)
$$
而因为深度学习常常一个batch一个batch，而不是一个数据一个数据输入，所以最终损失函数和结果也是以batch形式呈现的。batch之间没有关系，但是带上batch，损失函数就这样了(m为batch size）：
$$
J(\theta,b)=\frac{1}{m}\sum_{i=1}^{m}J(\theta,b;x^i,y^i)
$$


### 1.2 反向传播



### 1.3 梯度更新





## 2.深度学习常见技巧

### 2.0 对症下药：常见的问题

梯度消失，梯度爆炸，欠拟合，过拟合



### 2.1 初始化策略



### 2.2 激活函数选择



### 2.3 Batch Normalization



### 2.4 Dropout



### 2.5 Weight Decay



### 2.6 Learning Rate Decay



### 2.7 优化算法



### 2.8 训练时的一些好习惯

Early Stopping，先过拟合小batch，训练-测试-验证集的选择等



## 3.优化算法

### 3.1 基本优化：梯度下降



### 3.2 SGD--随机化



### 3.3 Momentum，Nestrov Momentum



### 3.4 Adam系列



### 3.5 整体架构和适用范围等



## 4.最基本的单元：MLP

### 4.1 定义和基本性质



### 4.2 表达能力